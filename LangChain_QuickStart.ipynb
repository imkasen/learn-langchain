{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO7PIk0Y7fZ/rEbdSMBR2jz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LangChain QuickStart Using Google Gemini"],"metadata":{"id":"z8t9DM10vGgb"}},{"cell_type":"markdown","source":["根据 LangChain 官方的 [QuickStart](https://python.langchain.com/docs/get_started/quickstart/) 教程，在 Google Colab 中使用 Google Gemini API 学习 LangChain 基本操作。\n","\n","*这里选择使用 Google Gemini 是因为只需要一个 Google 账号就能免费使用 Gemini API，而获取 OpenAI Key 不管什么途径都比较麻烦且需要付费。*"],"metadata":{"id":"c1nfMYbyhMt2"}},{"cell_type":"markdown","source":["## Installation"],"metadata":{"id":"owT0VPMbvnCD"}},{"cell_type":"code","source":["%pip install --upgrade --quiet langchain"],"metadata":{"id":"Pg2kPSohvoCv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["以上命令会安装 LangChain 的最基本依赖。如果想要将 LangChain 与大模型、数据库等进行集成，还需要额外单独安装指定的依赖库。"],"metadata":{"id":"7kekfIx2wPnU"}},{"cell_type":"markdown","source":["## Building with LangChain\n"],"metadata":{"id":"dk73ie9J1u8Z"}},{"cell_type":"markdown","source":["LangChain 使我们能够构建应用，这些应用能够把外部的数据源和计算资源接入到大型语言模型（LLM）。在本快速入门教程中，我们会介绍几种实现这一点的不同方法。首先，我们将介绍一个简单的 LLM 链。这个链条仅仅依赖于提示模板中的信息来给出反馈。其次，我们会构建一个检索链条，这个链条会从一个独立的数据库获取数据，并将其传送到提示模板中。之后，我们将加入聊天历史，创建一个支持对话的检索链条，让您能够以聊天的形式与 LLM 互动，并且 LLM 能够记住之前的提问。~~最终，我们将构建一个智能代理 —— 它会利用 LLM 判断在回答问题时是否需要去获取数据。~~"],"metadata":{"id":"gW5cYVyEhILB"}},{"cell_type":"markdown","source":["## LLM Chain"],"metadata":{"id":"2pwpnWZDvy2F"}},{"cell_type":"markdown","source":["在这里使用 Google Gemini API，先安装 Google AI 的 Python 库："],"metadata":{"id":"FjsZjs9Hw2NT"}},{"cell_type":"code","source":["%pip install --upgrade --quiet  langchain-google-genai"],"metadata":{"id":"wuRd0UoMvzwu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["从 Colab 中导入 Gemini API 密钥："],"metadata":{"id":"rlW6ZWX-xWee"}},{"cell_type":"code","source":["from google.colab import userdata\n","\n","API_KEY = userdata.get('API_KEY')"],"metadata":{"id":"zBRAWBiXv9tI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["之后让我们初始化模型，同时因为 `gemini-1.5-pro-latest` 仅允许每分钟 2 次查询，每天最多 1000 次查询，而 `gemini-pro` 允许每分钟 60 个请求，无查询总数限制，所以这里先用 `gemini-pro`："],"metadata":{"id":"FHcmRnlIxhz_"}},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAI\n","\n","llm = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=API_KEY)"],"metadata":{"id":"2EOzDo0Txiy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["我们来看看它对 LangSmith 是什么的解释 —— 这个概念在它的训练数据中不一定包含，因此我们可能不会得到一个很准确的答案。"],"metadata":{"id":"7arUzEd0yDch"}},{"cell_type":"code","source":["llm.invoke(\"how can langsmith help with testing?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"eUvdXjdpyHpg","executionInfo":{"status":"ok","timestamp":1715069918301,"user_tz":-480,"elapsed":7800,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"c3260574-941b-4742-b205-68080b2958fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"**Test Case Generation:**\\n\\n* **Natural Language Input:** Allows testers to write test cases in natural language, which Langsmith then translates into executable test scripts. This simplifies test case creation and reduces the risk of errors.\\n* **Automated Test Case Generation:** Uses AI algorithms to generate test cases based on requirements or existing code. This speeds up test case development and ensures comprehensive coverage.\\n* **Exploratory Testing:** Supports exploratory testing by providing language-based prompts to guide testers in discovering potential defects.\\n\\n**Test Execution:**\\n\\n* **Test Runner:** Executes test cases automatically or manually and captures results.\\n* **Parallel Execution:** Allows multiple test cases to be executed concurrently, reducing overall testing time.\\n* **Reporting:** Generates detailed test reports that include pass/fail status, execution time, and any errors or defects encountered.\\n\\n**Test Management:**\\n\\n* **Test Planning:** Helps testers organize and plan test cycles by creating test plans and defining test objectives.\\n* **Test Case Management:** Provides a centralized repository for managing test cases, including their status, description, and expected results.\\n* **Defect Tracking:** Integrates with defect tracking systems to allow testers to log and track defects identified during testing.\\n\\n**Integration with Other Tools:**\\n\\n* **CI/CD Pipelines:** Integrates with CI/CD tools to automate the testing process and provide continuous feedback.\\n* **DevOps:** Facilitates collaboration between development and testing teams by providing a shared platform for test case creation and execution.\\n* **Test Automation Frameworks:** Supports integration with popular test automation frameworks such as Selenium and Appium.\\n\\n**Additional Benefits:**\\n\\n* **Improved Test Coverage:** Langsmith's AI algorithms help identify and cover potential edge cases that might be missed during manual testing.\\n* **Reduced Testing Time:** Automating test case generation and execution significantly reduces the time spent on testing, freeing up testers for more exploratory and critical tasks.\\n* **Increased Test Quality:** Langsmith's natural language processing capabilities ensure that test cases are clear, unambiguous, and easily understood by both testers and developers.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["我们还可以使用提示模板来指导它给出回答。这些提示模板能够将用户的初始输入优化成更适合 LLM 的输入形式。"],"metadata":{"id":"GQ9v59tlzCbp"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a world class technical documentation writer.\"),\n","    (\"user\", \"{input}\")\n","])"],"metadata":{"id":"ne8D1JJNzDzV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["现在我们可以将这些组合成一个简单的 LLM 链。"],"metadata":{"id":"mNOpRuqpzfBw"}},{"cell_type":"code","source":["chain = prompt | llm"],"metadata":{"id":"F2f0VrKqzfp0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["我们现在可以调用它并询问同样的问题。尽管它依然不知道答案，但它应该以更适合技术撰稿人的语气来回应！"],"metadata":{"id":"f9IyfZYGz4Mk"}},{"cell_type":"code","source":["chain.invoke({\"input\": \"how can langsmith help with testing?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"HoTTDQgbz47d","executionInfo":{"status":"ok","timestamp":1715069924377,"user_tz":-480,"elapsed":6078,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"2977e746-8d98-4156-c67c-1d1b126d049e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'**How Langsmith Can Assist with Testing**\\n\\n**1. Automated Test Case Generation:**\\n\\n* Generate comprehensive test cases from natural language requirements.\\n* Support various testing methodologies (e.g., black-box, white-box).\\n* Improve test coverage and reduce manual effort.\\n\\n**2. Test Case Management:**\\n\\n* Organize and manage test cases in a structured hierarchy.\\n* Track test execution status and results.\\n* Provide real-time visibility into testing progress.\\n\\n**3. Test Data Generation:**\\n\\n* Generate realistic and diverse test data based on specified constraints.\\n* Ensure data validity and consistency.\\n* Reduce the risk of data-related test failures.\\n\\n**4. Defect Reporting:**\\n\\n* Automatically generate defect reports based on test results.\\n* Capture detailed information about defects, including steps to reproduce.\\n* Facilitate efficient defect tracking and resolution.\\n\\n**5. Collaboration and Communication:**\\n\\n* Provide a central platform for testers, developers, and stakeholders to collaborate.\\n* Enable real-time communication and feedback.\\n* Improve transparency and coordination throughout the testing process.\\n\\n**6. Test Automation Script Generation:**\\n\\n* Convert test cases into executable test scripts for automated testing.\\n* Support various testing frameworks and tools.\\n* Reduce the time and effort required for script creation.\\n\\n**7. Regression Testing:**\\n\\n* Generate regression test cases based on code changes.\\n* Identify and prioritize tests that need to be re-executed.\\n* Ensure that new changes do not introduce defects.\\n\\n**Benefits of Using Langsmith for Testing:**\\n\\n* Improved test coverage and quality\\n* Reduced manual effort and time-to-market\\n* Enhanced collaboration and communication\\n* Increased test efficiency and accuracy\\n* Reduced risk of defects and errors'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["ChatModel 的输出（也就是这个 LLM 链的输出结果）是一条消息格式的内容。不过，在很多情况下，将输出结果作为普通文本字符串处理会更加方便。让我们加入一个简单的输出转换器，将聊天消息格式的输出转换为文本字符串。"],"metadata":{"id":"aPz3xQh00R5n"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","\n","output_parser = StrOutputParser()"],"metadata":{"id":"JbaoAaj40YoH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["我们现在可以将此添加到之前的链中："],"metadata":{"id":"t8C3nytd0eFG"}},{"cell_type":"code","source":["chain = prompt | llm | output_parser"],"metadata":{"id":"4vts12xV0aU-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["我们现在可以调用它并提出同样的问题。答案现在将是一个字符串（而不是 ChatMessage）。\n","\n","*当前使用 Gemini API 在 Colab 中这几次输出结果格式并没有什么区别，已经是字符串了。*"],"metadata":{"id":"-NwoJji50ndB"}},{"cell_type":"code","source":["chain.invoke({\"input\": \"how can langsmith help with testing?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"3newXDr20m2q","executionInfo":{"status":"ok","timestamp":1715069930601,"user_tz":-480,"elapsed":6225,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"76967479-263f-45c8-9837-47e43a2bac9a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"**Langsmith's Testing Capabilities for Technical Documentation**\\n\\n**Automated Testing:**\\n\\n* **Syntax checking:** Verifies the correctness of code snippets and examples in the documentation.\\n* **Link checking:** Ensures all links in the documentation are valid and lead to the intended destinations.\\n* **Image verification:** Checks if images are displayed correctly and have appropriate alt text.\\n* **Spell checking and grammar correction:** Identifies and corrects spelling and grammar errors.\\n\\n**Collaboration and Review:**\\n\\n* **Shared document editing:** Allows multiple stakeholders to collaborate on the documentation and track changes.\\n* **Integrated commenting:** Facilitates discussions and feedback on specific sections of the documentation.\\n* **Automated review tools:** Provides suggestions for clarity, conciseness, and adherence to style guidelines.\\n\\n**User Experience and Accessibility Testing:**\\n\\n* **Readability analysis:** Assesses the readability of the documentation and identifies areas for improvement.\\n* **Accessibility checking:** Ensures the documentation is accessible to users with disabilities, such as screen readers.\\n* **User testing:** Collects feedback from users to identify usability issues and optimize the documentation.\\n\\n**Customizable Testing:**\\n\\n* **Custom rules and checks:** Allows users to define their own testing rules tailored to their specific documentation requirements.\\n* **Integration with external tools:** Supports integration with other testing tools, such as Selenium and Cypress, for more comprehensive testing.\\n\\n**Additional Benefits:**\\n\\n* **Improved accuracy:** Reduces the risk of errors in the documentation.\\n* **Reduced testing time:** Automates repetitive testing tasks, freeing up time for more critical tasks.\\n* **Enhanced collaboration:** Facilitates efficient review and feedback processes.\\n* **Increased user satisfaction:** Delivers high-quality documentation that meets the needs of users.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## Retrieval Chain"],"metadata":{"id":"UNeOmTvp05H6"}},{"cell_type":"markdown","source":["为了准确地回答这个问题（\"how can langsmith help with testing?\"），我们需要向大语言模型（LLM）补充更多相关的信息。这可以通过信息检索来完成。当你手头的**数据太多**，不能直接全部传给大语言模型时，信息检索就显得尤为重要。你可以利用一个信息检索工具来筛选出与问题最相关的信息片段，并只将这些信息片段输入到模型中。\n","\n","在这个流程中，我们会从一个检索工具中找到相关的文档，接着把这些文档内容嵌入到提示语中。检索工具的后端可以是多种形式：比如 SQL 表格、网络资源等。但在本例中，我们打算创建一个向量存储空间，并将其作为检索工具。\n","\n","首先，我们需要准备好我们计划建立索引的数据。为了完成这一步骤，我们会利用 WebBaseLoader 这个工具。进行这个操作之前，需要先确保我们的环境中安装了 BeautifulSoup 这个库。"],"metadata":{"id":"Ngfy8DAz25HT"}},{"cell_type":"code","source":["%pip install --upgrade --quiet beautifulsoup4"],"metadata":{"id":"49JebU5t3cWK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["导入和使用 WebBaseLoader："],"metadata":{"id":"ZW605B-64saI"}},{"cell_type":"code","source":["from langchain_community.document_loaders import WebBaseLoader\n","\n","# loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")  # 404\n","loader = WebBaseLoader(\"https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide\")\n","\n","docs = loader.load()"],"metadata":{"id":"BP0fe16E45YZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["接下来，我们需要把数据索引进向量数据库。此过程需要借助一些关键组件：一个嵌入模型以及一个向量数据库。\n","\n","至于如何使用嵌入模型，这里还是使用 Gemini API 的方式进行："],"metadata":{"id":"MzSaQuoicxDe"}},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","\n","embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=API_KEY)"],"metadata":{"id":"vxQ0rO3ScxrL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["现在，我们可以使用这个嵌入模型来导入文档至一个向量数据库。为了方便起见，我们此处选用一个简易的本地向量数据库：Facebook AI Similarity Search (FAISS)。\n","\n","首先，我们需要安装必要的软件包："],"metadata":{"id":"8xVS5tRWeRzk"}},{"cell_type":"code","source":["%pip install --upgrade --quiet faiss-cpu"],"metadata":{"id":"Aw7gUM4OeSmt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["接下来，我们将创建我们的向量索引："],"metadata":{"id":"zQD0YeOAegTg"}},{"cell_type":"code","source":["from langchain_community.vectorstores import FAISS\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter()\n","documents = text_splitter.split_documents(docs)\n","vector = FAISS.from_documents(documents, embeddings)"],"metadata":{"id":"9W19YoyAeg7B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["现在，我们已经将数据索引入向量数据库中，下一步是构建一个检索链。此链会处理传入的问题，搜索相关的文档，随后将这些文档连同原先的提问一起提交给 LLM，让它来回答原始的问题。\n","\n","首先，我们来搭建一个链，这个链能够处理用户提出的问题并根据检索到的文档生成回答。"],"metadata":{"id":"CL4Ph667fZsh"}},{"cell_type":"code","source":["from langchain.chains.combine_documents import create_stuff_documents_chain\n","\n","prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n","\n","<context>\n","{context}\n","</context>\n","\n","Question: {input}\"\"\")\n","\n","document_chain = create_stuff_documents_chain(llm, prompt)"],"metadata":{"id":"rA1ou1OIfabt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["如果我们想的话，可以通过直接输入文档来亲自执行这一过程："],"metadata":{"id":"bpjLy0nAf1Yz"}},{"cell_type":"code","source":["from langchain_core.documents import Document\n","\n","document_chain.invoke({\n","    \"input\": \"how can langsmith help with testing?\",\n","    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n","})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"WBR0Hoo6f1xA","executionInfo":{"status":"ok","timestamp":1715069950651,"user_tz":-480,"elapsed":1096,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"da4fc4f9-1445-4a6c-ce63-0514c4f5c58d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Langsmith can visualize test results.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["然而，我们希望从我们刚刚设置的检索系统中得到文档。这样，我们就可以利用这个系统动态地筛选出针对特定问题最相关的文档，并用于生成回答。"],"metadata":{"id":"H7a87virgSIR"}},{"cell_type":"code","source":["from langchain.chains import create_retrieval_chain\n","\n","retriever = vector.as_retriever()\n","retrieval_chain = create_retrieval_chain(retriever, document_chain)"],"metadata":{"id":"meEeR9GfgSwH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["现在，我们可以调用该链。这将返回一个字典类型的数据 - LLM 提供的响应将在 `answer` 键下找到。"],"metadata":{"id":"xXbAtpsagrgK"}},{"cell_type":"code","source":["response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awTphgnugr7L","executionInfo":{"status":"ok","timestamp":1715069954904,"user_tz":-480,"elapsed":4255,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"96013d90-4461-4847-e37e-d8ba603f9f87"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input': 'how can langsmith help with testing?',\n"," 'context': [Document(page_content='LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3 captures\\n16 Feb 2024 - 24 Mar 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeb\\nMAR\\nApr\\n\\n\\n\\n\\n24\\n\\n\\n\\n\\n2023\\n2024\\n2025\\n\\n\\n\\n\\n\\n\\n\\nsuccess\\nfail\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n About this capture\\n\\n\\n\\n\\n\\n\\nCOLLECTED BY\\n\\n\\n\\nCollection: Save Page Now Outlinks\\n\\n\\n\\n\\nTIMESTAMPS\\n\\n\\n\\n\\n\\nThe Wayback Machine - https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n","  Document(page_content=\"This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.\\nEvery playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Was this page helpful?PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n","  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'})],\n"," 'answer': 'LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications. These test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.'}"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["print(response[\"answer\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RiZCdnLO3mpQ","executionInfo":{"status":"ok","timestamp":1715069954904,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"1adf873b-8c92-4d94-ce3f-f1bd74f1a893"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications. These test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.\n"]}]},{"cell_type":"markdown","source":["## Conversation Retrieval Chain"],"metadata":{"id":"sHjUPOUmhAZv"}},{"cell_type":"markdown","source":["我们迄今为止构建的这个流程仅能回答单一问题。大语言模型的主流应用领域之一就是搭建聊天机器人。那么，我们应该如何改进这个流程，使它可以处理连续的对话问题呢？\n","\n","我们可以继续使用 `create_retrieval_chain` 函数，但需要做出两方面的调整：\n","\n","1. 我们的检索方法现在应综合考虑整个对话的历史信息，而不是只针对最近的一次提问。\n","2. 最终的 LLM 链，也需要将全部的历史信息纳入考虑。\n","\n","为了改进检索过程，我们需创建一个新的链。这个链会处理最新的输入（input）与过往的对话内容（chat_history），并借助 LLM 生成搜索问题。"],"metadata":{"id":"d9WYmMGNhBtF"}},{"cell_type":"code","source":["from langchain.chains import create_history_aware_retriever\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","# First we need a prompt that we can pass into an LLM to generate this search query\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"user\", \"{input}\"),\n","    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n","])\n","retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"],"metadata":{"id":"V2xm4TkYh-d1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["我们可以尝试一个场景，其中用户提出了一个后续问题，来验证这个流程的效果。"],"metadata":{"id":"DUJql3NTio3f"}},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage, AIMessage\n","\n","chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n","response = retriever_chain.invoke({\n","    \"chat_history\": chat_history,\n","    \"input\": \"Tell me how\"\n","})\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acF4_AoMipOU","executionInfo":{"status":"ok","timestamp":1715069956503,"user_tz":-480,"elapsed":1600,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"d9cb8244-4aab-455d-84bf-59ad4359b31e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n"," Document(page_content='LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3 captures\\n16 Feb 2024 - 24 Mar 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeb\\nMAR\\nApr\\n\\n\\n\\n\\n24\\n\\n\\n\\n\\n2023\\n2024\\n2025\\n\\n\\n\\n\\n\\n\\n\\nsuccess\\nfail\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n About this capture\\n\\n\\n\\n\\n\\n\\nCOLLECTED BY\\n\\n\\n\\nCollection: Save Page Now Outlinks\\n\\n\\n\\n\\nTIMESTAMPS\\n\\n\\n\\n\\n\\nThe Wayback Machine - https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n"," Document(page_content=\"This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.\\nEvery playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Was this page helpful?PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'})]"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["你会发现，这样做检索出了有关于 LangSmith 测试的文档。这是因为 LLM 根据聊天的历史内容和追加的问题，生成了一条新的搜索命令。\n","\n","既然我们已经拥有了这个新的检索工具，我们可以基于这些得到的文档，构建一条新的流程，以此继续进行对话。"],"metadata":{"id":"2pGge6RLi6K4"}},{"cell_type":"code","source":["prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"user\", \"{input}\"),\n","])\n","document_chain = create_stuff_documents_chain(llm, prompt)\n","\n","retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"],"metadata":{"id":"x0GiMVHZi77o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["现在，我们可以进行一次完整的测试："],"metadata":{"id":"6K7gXC6cjNAH"}},{"cell_type":"code","source":["chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n","response = retrieval_chain.invoke({\n","    \"chat_history\": chat_history,\n","    \"input\": \"Tell me how\"\n","})\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RF9gQSpHjNep","executionInfo":{"status":"ok","timestamp":1715069963864,"user_tz":-480,"elapsed":7362,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"50b7cd9d-2b8b-49a3-f9b4-d0fd1b4ff2a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n","  AIMessage(content='Yes!')],\n"," 'input': 'Tell me how',\n"," 'context': [Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n","  Document(page_content='LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3 captures\\n16 Feb 2024 - 24 Mar 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeb\\nMAR\\nApr\\n\\n\\n\\n\\n24\\n\\n\\n\\n\\n2023\\n2024\\n2025\\n\\n\\n\\n\\n\\n\\n\\nsuccess\\nfail\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n About this capture\\n\\n\\n\\n\\n\\n\\nCOLLECTED BY\\n\\n\\n\\nCollection: Save Page Now Outlinks\\n\\n\\n\\n\\nTIMESTAMPS\\n\\n\\n\\n\\n\\nThe Wayback Machine - https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n","  Document(page_content=\"This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.\\nEvery playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Was this page helpful?PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://web.archive.org/web/20240324062817/https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'})],\n"," 'answer': \"LangSmith provides multiple ways to test your LLM applications:\\n- Initial Test Set: Create datasets with collections of inputs and reference outputs to run tests on your LLM applications.\\n- Comparison View: View results for different configurations on the same data points side-by-side to track and diagnose regressions in test scores across multiple revisions of your application.\\n- Playground: Quickly test out different prompts and models and compare different runs.\\n- Beta Testing: Collect more data on how your LLM applications are performing in real-world scenarios and develop an understanding of the types of inputs the app is performing well or poorly on.\\n- Capturing Feedback: Gather human feedback on the responses your application is producing to highlight edge cases that are causing problematic responses.\\n- Annotating Traces: Send runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria.\\n- Adding Runs to a Dataset: Continue collecting data to refine and improve your application's performance by adding runs as examples to datasets.\\n- Monitoring and A/B Testing: Track key metrics over time and drill down into specific data points to get a trace table for that time period. Mark different versions of your applications with different identifiers and view how they are performing side-by-side within each chart.\"}"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["print(response['answer'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5NKOLvj5ZSc","executionInfo":{"status":"ok","timestamp":1715069963864,"user_tz":-480,"elapsed":5,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"13477df1-7e78-453b-ae8d-df6500372943"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LangSmith provides multiple ways to test your LLM applications:\n","- Initial Test Set: Create datasets with collections of inputs and reference outputs to run tests on your LLM applications.\n","- Comparison View: View results for different configurations on the same data points side-by-side to track and diagnose regressions in test scores across multiple revisions of your application.\n","- Playground: Quickly test out different prompts and models and compare different runs.\n","- Beta Testing: Collect more data on how your LLM applications are performing in real-world scenarios and develop an understanding of the types of inputs the app is performing well or poorly on.\n","- Capturing Feedback: Gather human feedback on the responses your application is producing to highlight edge cases that are causing problematic responses.\n","- Annotating Traces: Send runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria.\n","- Adding Runs to a Dataset: Continue collecting data to refine and improve your application's performance by adding runs as examples to datasets.\n","- Monitoring and A/B Testing: Track key metrics over time and drill down into specific data points to get a trace table for that time period. Mark different versions of your applications with different identifiers and view how they are performing side-by-side within each chart.\n"]}]},{"cell_type":"markdown","source":["我们可以看到，这一系列动作让我们得到了一个条理清晰的答案——这意味着，我们已经成功地把检索流程转化为了一个聊天机器人！"],"metadata":{"id":"FxhIEs_1jalV"}},{"cell_type":"markdown","source":["## Agent"],"metadata":{"id":"63bhUtkzjdj7"}},{"cell_type":"markdown","source":["使用 Gemini API 来创建 Agent 过程要比官方基于 OpenAI 的示例复杂，具体代码可以参考 [Gemini Agent Example](https://github.com/MikeChan-HK/Gemini-agent-example)，这里先暂时跳过。"],"metadata":{"id":"vK-GtAYwtqTI"}},{"cell_type":"markdown","source":["## Serving with LangServe"],"metadata":{"id":"pnmyTdKLtwzf"}},{"cell_type":"markdown","source":["暂时先不学习 LangSmith 和 LangServe，跳过。"],"metadata":{"id":"ZHElXzlXtyyY"}}]}