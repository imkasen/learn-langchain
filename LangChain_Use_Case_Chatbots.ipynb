{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPeeRBVVXVaGKRQNgGRR7co"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Use Case: Chatbots QuickStart"],"metadata":{"id":"9-ySl1gqMoXG"}},{"cell_type":"markdown","source":["我们将举例说明如何设计和实现由 LLM 驱动的聊天机器人。以下是我们将使用的一些高级组件：\n","\n","- `Chat Models`：聊天机器人界面基于信息而非原始文本，因此最适合 Chat Models 而非 LLMs。\n","- `Prompt Templates`：它简化了将默认信息、用户输入、聊天记录和（可选）其他检索到的上下文组合在一起的提示过程。\n","- `Chat History`：它允许聊天机器人 “记住” 过去的互动，并在回答后续问题时将其考虑在内。\n","- `Retrievers`：如果您想构建一个可以使用特定领域的最新知识作为上下文来增强其响应的聊天机器人，这将非常有用。"],"metadata":{"id":"7t1dPWoELvaV"}},{"cell_type":"markdown","source":["## QuickStart"],"metadata":{"id":"8m0bsX4cNgvW"}},{"cell_type":"code","source":["%pip install --upgrade --quiet langchain langchain-google-genai"],"metadata":{"id":"e7dtuVv2NIAM","executionInfo":{"status":"ok","timestamp":1715157128002,"user_tz":-480,"elapsed":8829,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","\n","API_KEY = userdata.get('API_KEY')"],"metadata":{"id":"kYIgqQHVNona","executionInfo":{"status":"ok","timestamp":1715157132136,"user_tz":-480,"elapsed":1982,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["让我们初始化聊天模型：\n","\n","*`gemini-pro` 目前不支持 `SystemMessage`，但可以将其添加到该行的第一条人类信息中。如果需要这种行为，只需将 `convert_system_message_too_human` 设置为 `True` 即可。*"],"metadata":{"id":"zzWzy92rOAoG"}},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","# chat = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=API_KEY, convert_system_message_to_human=True)\n","chat = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=API_KEY)"],"metadata":{"id":"LegeDpNTN0f4","executionInfo":{"status":"ok","timestamp":1715157145746,"user_tz":-480,"elapsed":1976,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["如果我们调用聊天模型，输出结果是 `AIMessage`："],"metadata":{"id":"aO-H5j7XOUNb"}},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","\n","response = chat.invoke(\n","    [\n","        HumanMessage(\n","            content=\"Translate this sentence from English to French: I love programming.\"\n","        )\n","    ]\n",")\n","response"],"metadata":{"id":"vQKwGSfEOGmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715157151449,"user_tz":-480,"elapsed":1142,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"011dea76-bf86-4c3e-d4cd-c0e3b606ebe0"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"J'adore programmer. \\n\", response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-7c90f01a-2e0b-44b3-96b0-e5eeba0cdc59-0')"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["print(response.content)"],"metadata":{"id":"SuUGnMYPOikw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715157155017,"user_tz":-480,"elapsed":314,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"94dbd060-a34f-4cf5-dd9f-daed686f96e4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["J'adore programmer. \n","\n"]}]},{"cell_type":"markdown","source":["模型本身并没有状态的概念。例如，如果你提出一个后续问题："],"metadata":{"id":"aivtgG-dOxJJ"}},{"cell_type":"code","source":["response = chat.invoke([HumanMessage(content=\"What did you just say?\")])\n","print(response.content)"],"metadata":{"id":"1tcGLDMwOxiW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715157199510,"user_tz":-480,"elapsed":2624,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"a889d5ae-b192-45b7-f633-1170521fdb12"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["I introduced myself as Gemini, a large language model created by Google. I also mentioned that my knowledge is only current up to November 2023 and that I will try my best to follow your instructions while prioritizing safety. Is there anything specific you would like to know or have me do? \n","\n"]}]},{"cell_type":"markdown","source":["我们可以看到，它没有将之前的对话转折纳入上下文，因此无法回答问题。\n","\n","为了解决这个问题，我们需要将整个对话历史记录传入模型。让我们看看这样做会发生什么："],"metadata":{"id":"kgd_i-EWPDwH"}},{"cell_type":"code","source":["from langchain_core.messages import AIMessage\n","\n","response = chat.invoke(\n","    [\n","        HumanMessage(\n","            content=\"Translate this sentence from English to French: I love programming.\"\n","        ),\n","        AIMessage(content=\"J'adore la programmation.\"),\n","        HumanMessage(content=\"What did you just say?\"),\n","    ]\n",")\n","print(response.content)"],"metadata":{"id":"32HBvmR4PENZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715157212862,"user_tz":-480,"elapsed":1729,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"fdc11392-8da7-48b5-e815-1086f6eac415"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["I said, \"J'adore la programmation,\" which is French for \"I love programming.\" \n","\n"]}]},{"cell_type":"markdown","source":["现在我们可以看到，我们得到了很好的回应！\n","\n","这就是聊天机器人进行对话互动的基本理念。"],"metadata":{"id":"MayXs9mUPbKf"}},{"cell_type":"markdown","source":["## Prompt templates"],"metadata":{"id":"nMUSMbzpPenc"}},{"cell_type":"markdown","source":["让我们定义一个提示模板，使格式化更容易一些。我们可以通过在模型中插入管道来创建一个链："],"metadata":{"id":"84609sTTPtTe"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n","        ),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","    ]\n",")\n","\n","chain = prompt | chat"],"metadata":{"id":"BQ9mY4fFRTWr","executionInfo":{"status":"ok","timestamp":1715157218188,"user_tz":-480,"elapsed":292,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["上面的 `MessagesPlaceholder` 将聊天信息作为 `chat_history` 直接插入链的输入提示中。然后，我们可以像这样调用链："],"metadata":{"id":"19Lf9mOVRb1Y"}},{"cell_type":"code","source":["response = chain.invoke(\n","    {\n","        \"messages\": [\n","            HumanMessage(\n","                content=\"Translate this sentence from English to French: I love programming.\"\n","            ),\n","            AIMessage(content=\"J'adore la programmation.\"),\n","            HumanMessage(content=\"What did you just say?\"),\n","        ],\n","    }\n",")\n","print(response.content)"],"metadata":{"id":"XM3HiW6qR4Vi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715157225050,"user_tz":-480,"elapsed":1669,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"b12b41c6-08e4-479d-e266-48b9abc59d1d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["I said, \"J'adore la programmation,\" which is French for \"I love programming.\" \n","\n"]}]},{"cell_type":"markdown","source":["## Message history"],"metadata":{"id":"MFcjcEaGR9t4"}},{"cell_type":"markdown","source":["为了更方便地管理聊天记录，我们可以使用一个叫做 `MessageHistory` 的类，它负责储存和加载聊天消息。尽管有很多内置功能可以将聊天消息长期保存到各种数据库中，但是在这个快速入门中，我们将使用一个保存在内存中的、用于演示的聊天消息历史记录，也就是 `ChatMessageHistory`。"],"metadata":{"id":"VM0VTMGn9o-7"}},{"cell_type":"code","source":["from langchain.memory import ChatMessageHistory\n","\n","demo_ephemeral_chat_history = ChatMessageHistory()\n","demo_ephemeral_chat_history.add_user_message(\"hi!\")\n","demo_ephemeral_chat_history.add_ai_message(\"whats up?\")\n","demo_ephemeral_chat_history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLfpWXym94pO","executionInfo":{"status":"ok","timestamp":1715157237927,"user_tz":-480,"elapsed":734,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"bccad778-cd3b-41ab-9c19-0fe0b7baa8ae"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["InMemoryChatMessageHistory(messages=[HumanMessage(content='hi!'), AIMessage(content='whats up?')])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["demo_ephemeral_chat_history.messages"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2Xwsk9yJ70I","executionInfo":{"status":"ok","timestamp":1715157240368,"user_tz":-480,"elapsed":353,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"0bcae145-17a6-465c-f960-b3c9dafb0cd6"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["我们可以将存储的消息直接传递到我们的链中作为参数："],"metadata":{"id":"t4KR2YX9-IvO"}},{"cell_type":"code","source":["demo_ephemeral_chat_history.add_user_message(\n","    \"Translate this sentence from English to French: I love programming.\"\n",")\n","\n","response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n","print(response.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dUAMYfR-JGr","executionInfo":{"status":"ok","timestamp":1715157244170,"user_tz":-480,"elapsed":1744,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"60468dc9-6d9a-4938-b6af-13cbbb49348e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["J'adore la programmation. \n","\n"]}]},{"cell_type":"code","source":["demo_ephemeral_chat_history.add_ai_message(response)\n","demo_ephemeral_chat_history.add_user_message(\"What did you just say?\")\n","\n","response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n","print(response.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqQYHHgfB8yZ","executionInfo":{"status":"ok","timestamp":1715157277575,"user_tz":-480,"elapsed":308,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"ba018d76-7f4b-4d0d-f514-2cdc118cde1d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["I said, \"J'adore la programmation,\" which is French for \"I love programming.\"\n","\n"]}]},{"cell_type":"markdown","source":["现在，我们有了一个基本的聊天机器人！\n","\n","虽然仅凭模型的内部知识，这个链条本身就能成为一个有用的聊天机器人，但在特定领域知识之上引入某种形式的 “retrieval-augmented generation”（简称 RAG）往往很有用，能让我们的聊天机器人更有针对性。接下来我们将介绍这一点。"],"metadata":{"id":"qDz98fdDCmP_"}},{"cell_type":"markdown","source":["## Retrievers"],"metadata":{"id":"zADMGgQXC0nl"}},{"cell_type":"markdown","source":["我们可以设置并使用检索器为聊天机器人获取特定领域的知识。为了展示这一点，让我们扩展上面创建的简单聊天机器人，使其能够回答有关 LangSmith 的问题。\n","\n","我们将使用 LangSmith 文档作为源材料，并将其存储在矢量数据库中，以便日后检索。请注意，本示例将略去解析和存储数据源的一些具体细节。\n","\n","让我们设置我们的检索器。首先，我们将安装一些必需的依赖项："],"metadata":{"id":"bl43aVf8C1Ro"}},{"cell_type":"code","source":["%pip install --upgrade --quiet langchain-chroma beautifulsoup4"],"metadata":{"id":"PUTPAQhwDT6Z","executionInfo":{"status":"ok","timestamp":1715157304745,"user_tz":-480,"elapsed":10041,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["接下来，我们将使用文档加载器从网页中提取数据："],"metadata":{"id":"EAjDntK4DdgF"}},{"cell_type":"code","source":["from langchain_community.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\"https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview\")\n","data = loader.load()"],"metadata":{"id":"ZjzokO1GDd2h","executionInfo":{"status":"ok","timestamp":1715157355375,"user_tz":-480,"elapsed":1818,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["接下来，我们将其分割成 LLM 的上下文窗口可以处理的较小块，并将其存储到矢量数据库中："],"metadata":{"id":"rrjZf0fVD6UU"}},{"cell_type":"code","source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n","all_splits = text_splitter.split_documents(data)"],"metadata":{"id":"IwgzCAeND8V_","executionInfo":{"status":"ok","timestamp":1715157359442,"user_tz":-480,"elapsed":297,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["然后，我们将这些块嵌入并存储到一个矢量数据库中："],"metadata":{"id":"MouBTgAJEg8W"}},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","\n","embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=API_KEY)\n","vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"],"metadata":{"id":"jcT7YXKbEhPa","executionInfo":{"status":"ok","timestamp":1715157365173,"user_tz":-480,"elapsed":1943,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["最后，让我们从初始化的 `vectorstore` 中创建一个检索器："],"metadata":{"id":"loqzPq2oFHXE"}},{"cell_type":"code","source":["# k is the number of chunks to retrieve\n","retriever = vectorstore.as_retriever(k=4)\n","\n","docs = retriever.invoke(\"how can langsmith help with testing?\")\n","docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njL_5AE7FJqb","executionInfo":{"status":"ok","timestamp":1715157372303,"user_tz":-480,"elapsed":818,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"c5886255-4f34-4ab8-856d-8e87b2adf741"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we‚Äôve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n"," Document(page_content='for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation‚ÄãInitially, we do most of our evaluation manually and ad hoc. We pass in different', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n"," Document(page_content='assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We‚Äôve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing ‚Äî', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n"," Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["我们可以看到，调用上述检索器后，LangSmith 文档的某些部分包含了测试信息，聊天机器人在回答问题时可以将这些信息作为上下文。"],"metadata":{"id":"iFo9FVEdFV-U"}},{"cell_type":"markdown","source":["### Handling documents"],"metadata":{"id":"aDRuxVF1FWn8"}},{"cell_type":"markdown","source":["让我们修改之前的提示词，以接受文档作为上下文。我们将使用 `create_stuff_documents_chain` 辅助函数将所有输入文档 “填充” 到提示中，该函数还能方便地处理格式化。我们使用 `ChatPromptTemplate.from_messages()` 方法来格式化要传递给模型的消息输入，包括直接注入聊天历史消息的 `MessagesPlaceholder()` 方法："],"metadata":{"id":"HWTGrA0aFYqx"}},{"cell_type":"code","source":["from langchain.chains.combine_documents import create_stuff_documents_chain\n","\n","question_answering_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n","        ),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","    ]\n",")\n","\n","document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"],"metadata":{"id":"Psg5F4JpFt-s","executionInfo":{"status":"ok","timestamp":1715157389187,"user_tz":-480,"elapsed":306,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["我们可以用上面获取的原始文件来调用这个 `document_chain` ："],"metadata":{"id":"9azuhUS3GFDh"}},{"cell_type":"code","source":["from langchain.memory import ChatMessageHistory\n","\n","demo_ephemeral_chat_history = ChatMessageHistory()\n","demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n","\n","response = document_chain.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history.messages,\n","        \"context\": docs,\n","    }\n",")\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"ZOdfQGc2GFiB","executionInfo":{"status":"ok","timestamp":1715157405576,"user_tz":-480,"elapsed":12062,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"453e1cdc-d4b9-4751-84d1-2013dec5086b"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"## LangSmith and Testing: A Powerful Combination\\n\\nLangSmith offers several features that can significantly enhance your testing process, making it more efficient and effective. Here's how: \\n\\n**1. Data Generation and Augmentation:**\\n\\n*   **Generating Test Cases:** LangSmith can help you create diverse and realistic test cases by generating text that follows specific patterns or criteria. This is particularly useful for testing NLP models, chatbots, and other language-based systems. \\n*   **Data Augmentation:** Expanding your existing test data set is crucial for robust testing. LangSmith can augment your data by paraphrasing, translating, or generating similar examples, improving your model'sgeneralizability.\\n\\n**2. Identifying Bias and Errors:**\\n\\n*   **Bias Detection:** LangSmith can analyze your training and testing data to identify potential biases. This helps ensure your models are fair and unbiased, leading to more reliable results.\\n*   **Error Analysis:**  By analyzing model outputs and comparing them to expected outcomes, LangSmith can help pinpoint specific areas where your model is underperforming. This allows you to focus your efforts on improving the model's weaknesses. \\n\\n**3. Evaluating Model Performance:**\\n\\n*   **Metrics Calculation:** LangSmith can automatically calculate various evaluation metrics for your models, such as accuracy, precision, recall, and F1 score. This provides quantitative insights into your model's performance.\\n*   **Comparative Analysis:** You can compare the performance of different models or model versions side-by-side, making it easier to choose the best-performing option for your specific task.\\n\\n**4. Streamlining the Testing Workflow:**\\n\\n*   **Integration with Testing Frameworks:** LangSmith can integrate with popular testing frameworks, allowing you to automate the testing process and save valuable time.\\n*   **Version Control and Collaboration:**  LangSmith facilitates collaboration among team members by providing version control for your test data and models. \\n\\n**Additional Benefits:**\\n\\n*   **Multilingual Support:** LangSmith supports multiple languages, making it suitable for testing models that operate in different linguistic contexts.\\n*   **Customization:** You can tailor LangSmith's features to your specific testing needs and preferences. \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["我们可以看到模型根据输入文件中的信息所合成的答案。"],"metadata":{"id":"4XKsbGyxGY5A"}},{"cell_type":"markdown","source":["### Creating a retrieval chain"],"metadata":{"id":"kPwtrc3dGa7S"}},{"cell_type":"markdown","source":["接下来，让我们将检索器集成到链中。我们的检索器应检索与我们从用户那里传递的最后一条消息相关的信息，因此我们提取它并将其用作输入来获取相关文档，然后将其作为上下文添加到当前链中。我们将上下文和先前的消息传递到文档链中以生成最终答案。\n","\n","我们还使用 `RunnablePassthrough.assign()` 方法在每次调用时传递中间步骤。下面就是它的样子："],"metadata":{"id":"j2_7z_q-Gc7Z"}},{"cell_type":"code","source":["demo_ephemeral_chat_history.messages"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCR8BWUCMlKv","executionInfo":{"status":"ok","timestamp":1715157414183,"user_tz":-480,"elapsed":616,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"02ea8cc3-f95a-4674-a397-74b301de961a"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='how can langsmith help with testing?')]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["from typing import Dict\n","\n","from langchain_core.runnables import RunnablePassthrough\n","\n","\n","def parse_retriever_input(params: Dict):\n","    return params[\"messages\"][-1].content\n","\n","\n","retrieval_chain = RunnablePassthrough.assign(\n","    context=parse_retriever_input | retriever,\n",").assign(\n","    answer=document_chain,\n",")\n","\n","response = retrieval_chain.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history.messages,\n","    }\n",")\n","\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1xhdem6Gwai","executionInfo":{"status":"ok","timestamp":1715157427191,"user_tz":-480,"elapsed":5723,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"13867e5e-bd2c-418d-99e2-721b7b704517"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'messages': [HumanMessage(content='how can langsmith help with testing?')],\n"," 'context': [Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we‚Äôve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation‚ÄãInitially, we do most of our evaluation manually and ad hoc. We pass in different', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We‚Äôve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing ‚Äî', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n"," 'answer': 'LangSmith offers several features to assist with testing your language model applications:\\n\\n* **Dataset Construction and Management:** You can either build datasets as you go or create smaller, targeted datasets manually. LangSmith helps manage these datasets for testing purposes.\\n* **Simplified Testing:** The platform allows for testing both the overall flow of your application and individual components like LLM Chain or Chat Model examples. This flexibility caters to different testing needs.\\n* **Evaluation and Feedback:** While initial evaluation might be manual, LangSmith provides tools to track performance over time, identify underperforming data points, and associate feedback with specific runs. This data can then be used to refine your datasets and improve future testing. \\n'}"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["print(response['answer'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_oo8OQwHt9g","executionInfo":{"status":"ok","timestamp":1715157440898,"user_tz":-480,"elapsed":412,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"7171ce26-73c8-41e1-a107-9a959351deb9"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["LangSmith offers several features to assist with testing your language model applications:\n","\n","* **Dataset Construction and Management:** You can either build datasets as you go or create smaller, targeted datasets manually. LangSmith helps manage these datasets for testing purposes.\n","* **Simplified Testing:** The platform allows for testing both the overall flow of your application and individual components like LLM Chain or Chat Model examples. This flexibility caters to different testing needs.\n","* **Evaluation and Feedback:** While initial evaluation might be manual, LangSmith provides tools to track performance over time, identify underperforming data points, and associate feedback with specific runs. This data can then be used to refine your datasets and improve future testing. \n","\n"]}]},{"cell_type":"code","source":["demo_ephemeral_chat_history.add_ai_message(response[\"answer\"])\n","demo_ephemeral_chat_history.add_user_message(\"tell me more about that!\")\n","\n","response = retrieval_chain.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history.messages,\n","    },\n",")\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0idtjJsoHYTu","executionInfo":{"status":"ok","timestamp":1715157469456,"user_tz":-480,"elapsed":9028,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"5685617b-2ee3-49d4-ab73-8e4a54896c8e"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'messages': [HumanMessage(content='how can langsmith help with testing?'),\n","  AIMessage(content='LangSmith offers several features to assist with testing your language model applications:\\n\\n* **Dataset Construction and Management:** You can either build datasets as you go or create smaller, targeted datasets manually. LangSmith helps manage these datasets for testing purposes.\\n* **Simplified Testing:** The platform allows for testing both the overall flow of your application and individual components like LLM Chain or Chat Model examples. This flexibility caters to different testing needs.\\n* **Evaluation and Feedback:** While initial evaluation might be manual, LangSmith provides tools to track performance over time, identify underperforming data points, and associate feedback with specific runs. This data can then be used to refine your datasets and improve future testing. \\n'),\n","  HumanMessage(content='tell me more about that!')],\n"," 'context': [Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content=\"most of these evaluators aren't perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation‚ÄãAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='following pain points:What was the exact input to the LLM?‚ÄãLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string ‚Üí string (or chat messages ‚Üí chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation‚ÄãInitially, we do most of our evaluation manually and ad hoc. We pass in different', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n"," 'answer': \"## LangSmith's Testing and Evaluation Features: A Deeper Dive\\n\\n**Dataset Management:**\\n\\n* **Flexibility:**  LangSmith allows you to use datasets built organically during development or create specific datasets for focused testing. \\n* **Organization:**  The platform helps you manage and organize these datasets, making it easy to access and utilize them for various testing scenarios.\\n\\n**Testing Options:**\\n\\n* **Holistic Testing:**  LangSmith enables testing of the entire application flow, ensuring all components work together seamlessly.\\n* **Modular Testing:**  You can also test individual modules like LLM Chain or Chat Model examples. This is useful for pinpointing issues within specific components and making targeted improvements.\\n\\n**Evaluation and Feedback Mechanisms:**\\n\\n* **Performance Tracking:**  LangSmith lets you monitor your application's performance over time, providing insights into trends and potential areas for improvement.\\n* **Data Point Identification:**  The platform helps identify underperforming data points, which can then be added to your datasets for further testing and refinement.\\n* **Feedback Integration:**  LangSmith allows you to associate user feedback (e.g., thumbs up/down) with specific runs. This feedback loop is valuable for understanding user satisfaction and pinpointing areas that require attention.\\n\\n**Human Review:**\\n\\nWhile LangSmith offers powerful tools for testing and evaluation, it's important to remember that human review remains crucial for ensuring the highest quality and reliability in your language model applications. \\n\"}"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["print(response['answer'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUAAEt0iNeJ4","executionInfo":{"status":"ok","timestamp":1715157473410,"user_tz":-480,"elapsed":420,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"a6ceb74b-8f9d-4285-f7cc-3867ea456551"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["## LangSmith's Testing and Evaluation Features: A Deeper Dive\n","\n","**Dataset Management:**\n","\n","* **Flexibility:**  LangSmith allows you to use datasets built organically during development or create specific datasets for focused testing. \n","* **Organization:**  The platform helps you manage and organize these datasets, making it easy to access and utilize them for various testing scenarios.\n","\n","**Testing Options:**\n","\n","* **Holistic Testing:**  LangSmith enables testing of the entire application flow, ensuring all components work together seamlessly.\n","* **Modular Testing:**  You can also test individual modules like LLM Chain or Chat Model examples. This is useful for pinpointing issues within specific components and making targeted improvements.\n","\n","**Evaluation and Feedback Mechanisms:**\n","\n","* **Performance Tracking:**  LangSmith lets you monitor your application's performance over time, providing insights into trends and potential areas for improvement.\n","* **Data Point Identification:**  The platform helps identify underperforming data points, which can then be added to your datasets for further testing and refinement.\n","* **Feedback Integration:**  LangSmith allows you to associate user feedback (e.g., thumbs up/down) with specific runs. This feedback loop is valuable for understanding user satisfaction and pinpointing areas that require attention.\n","\n","**Human Review:**\n","\n","While LangSmith offers powerful tools for testing and evaluation, it's important to remember that human review remains crucial for ensuring the highest quality and reliability in your language model applications. \n","\n"]}]},{"cell_type":"markdown","source":["聊天机器人现在可以用对话的方式回答特定领域的问题了。\n","\n","顺便说一句，如果不想返回所有中间步骤，您可以使用管道直接进入文档链来定义检索链，而不是最后的 `.assign()` 调用："],"metadata":{"id":"qxiNt3RLH2UR"}},{"cell_type":"code","source":["retrieval_chain_with_only_answer = (\n","    RunnablePassthrough.assign(\n","        context=parse_retriever_input | retriever,\n","    )\n","    | document_chain\n",")\n","\n","response = retrieval_chain_with_only_answer.invoke(\n","    {\n","        \"messages\": demo_ephemeral_chat_history.messages,\n","    },\n",")\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"pXU1yr-iH9WV","executionInfo":{"status":"ok","timestamp":1715157534262,"user_tz":-480,"elapsed":13028,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"54c748c3-eb8d-4cf9-b5bf-a1c0aae1eb33"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"## LangSmith's Testing and Evaluation Features: A Deeper Dive\\n\\nHere's a closer look at how LangSmith can assist you in testing and evaluating your language model applications:\\n\\n**Dataset Management:**\\n\\n* **Building Datasets:** LangSmith allows you to create datasets as you work, accumulating data points over time. Alternatively, you can build smaller, more focused datasets manually for specific testing scenarios.\\n* **Organization and Tracking:**  The platform helps you keep your datasets organized and readily accessible for testing purposes. This includes features for labeling, filtering, and searching your data.\\n\\n**Testing Flexibility:**\\n\\n* **End-to-End Testing:** LangSmith enables you to test the complete flow of your application, ensuring all components work together seamlessly. This is crucial for identifying any integration issues or bottlenecks in the overall user experience.\\n* **Modular Testing:** The platform also supports testing individual components like LLM Chain or Chat Model examples. This allows you to focus on specific functionalities and isolate potential problems within those modules.\\n\\n**Evaluation and Feedback:**\\n\\n* **Performance Tracking:** LangSmith provides tools for monitoring the performance of your language model over time. You can track metrics such as accuracy, fluency, and relevance to assess how well your model is meeting its objectives.\\n* **Identifying Issues:** The platform helps you pinpoint underperforming data points or areas where your model struggles. This allows you to focus your efforts on improving specific aspects of the model's behavior.\\n* **Feedback Association:**  LangSmith enables you to associate feedback with specific runs and data points. This feedback can be used to refine your datasets, adjust model parameters, and guide further development efforts.\\n\\n**Additional Considerations:**\\n\\n* **Human Review Remains Essential:** While LangSmith provides valuable tools for automated evaluation, human review is still crucial for ensuring the highest quality and reliability in your application. \\n* **Evaluator Limitations:** The automatic evaluation metrics offered by LangSmith are not perfect and should not be blindly trusted. However, they serve as useful guides to direct your attention to specific examples that require closer examination.\\n* **Scaling Efficiency:** As the number of data points increases, manual evaluation becomes impractical. LangSmith's tools become increasingly valuable in these situations, helping you efficiently manage and analyze large datasets. \\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## Query transformation"],"metadata":{"id":"pabniGk8IKEA"}},{"cell_type":"markdown","source":["在上面的示例中，当我们提出一个后续问题 “tell me more about that!” 时，你可能会注意到，检索到的文档并没有直接包含有关测试的信息。\n","\n","*跟模型有关系，上面在使用 gemini 1.5 pro 时可以继续获得有关测试的信息。*\n","\n","这是因为我们将 “tell me more about that!” 作为查询逐字传递给了检索器。检索链中的输出仍然可行，因为文档链检索链可以根据聊天记录生成答案，但我们可以检索到内容更丰富、信息量更大的文档。\n","\n","为了解决这个常见问题，让我们添加一个 `query transformation` 步骤。我们将对旧的检索器进行如下封装："],"metadata":{"id":"zPocD4iJIK0S"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableBranch\n","\n","# We need a prompt that we can pass into an LLM to generate a transformed search query\n","\n","query_transform_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","        (\n","            \"user\",\n","            \"Given the above conversation, generate a search query to look up in order to\"\n","            \"get information relevant to the conversation. Only respond with the query, nothing else.\",\n","        ),\n","    ]\n",")\n","\n","query_transforming_retriever_chain = RunnableBranch(\n","    (\n","        lambda x: len(x.get(\"messages\", [])) == 1,\n","        # If only one message, then we just pass that message's content to retriever\n","        (lambda x: x[\"messages\"][-1].content) | retriever,\n","    ),\n","    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n","    query_transform_prompt | chat | StrOutputParser() | retriever,\n",").with_config(run_name=\"chat_retriever_chain\")"],"metadata":{"id":"9IxEhCoWI5-S","executionInfo":{"status":"ok","timestamp":1715157723512,"user_tz":-480,"elapsed":394,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["现在，让我们用这个新的 `query_transforming_retriever_chain` 来重新创建之前的链。请注意，这个新链接受一个 dict 作为输入，并解析出一个字符串传递给检索器，因此我们无需在顶层进行额外的解析："],"metadata":{"id":"-PNvzVAIQtAr"}},{"cell_type":"code","source":["document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n","\n","conversational_retrieval_chain = RunnablePassthrough.assign(\n","    context=query_transforming_retriever_chain,\n",").assign(\n","    answer=document_chain,\n",")\n","\n","demo_ephemeral_chat_history = ChatMessageHistory()"],"metadata":{"id":"fGdwVYnFQt_W","executionInfo":{"status":"ok","timestamp":1715157728122,"user_tz":-480,"elapsed":297,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n","\n","response = conversational_retrieval_chain.invoke(\n","    {\"messages\": demo_ephemeral_chat_history.messages},\n",")\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4xn6wAdqQ2IV","executionInfo":{"status":"ok","timestamp":1715157738410,"user_tz":-480,"elapsed":8082,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"e28b328d-84a9-4d4e-fa6a-f24da4c582e3"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'messages': [HumanMessage(content='how can langsmith help with testing?')],\n"," 'context': [Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we‚Äôve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation‚ÄãInitially, we do most of our evaluation manually and ad hoc. We pass in different', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We‚Äôve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing ‚Äî', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n"," 'answer': \"## LangSmith's Role in Testing LLMs\\n\\nWhile LangSmith doesn't directly execute tests, it offers valuable tools to guide and enhance your LLM testing process, especially when dealing with large datasets.  Here's how:\\n\\n**Focusing Your Evaluation Efforts:**\\n\\n*   **Highlighting Potential Issues:** LangSmith's automatic evaluation metrics help identify potential problems or inconsistencies in your LLM's output. This allows you to prioritize manual review of specific examples rather than sifting through the entire dataset.\\n*   **Managing Complexity:**  LLM calls often involve complex combinations of user input, templates, and auxiliary functions. LangSmith helps you understand the exact input provided to the LLM, making it easier to pinpoint the source of any issues.\\n\\n**Improving Test Coverage:**\\n\\n*   **Testing Different Flows:** LangSmith supports testing both the overall flow of your application and individual LLM Chain or Chat Model components. This ensures comprehensive evaluation of your LLM's performance in various contexts.\\n*   **Scaling with Data Volume:** As your dataset grows, manual evaluation becomes impractical. LangSmith's tools enable you to efficiently manage and analyze large amounts of data, ensuring consistent and scalable testing. \\n\"}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["print(response['answer'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYB6jFk5RtMs","executionInfo":{"status":"ok","timestamp":1715157740421,"user_tz":-480,"elapsed":2,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"06b82f35-469b-4572-e1b1-3fccb444c8de"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["## LangSmith's Role in Testing LLMs\n","\n","While LangSmith doesn't directly execute tests, it offers valuable tools to guide and enhance your LLM testing process, especially when dealing with large datasets.  Here's how:\n","\n","**Focusing Your Evaluation Efforts:**\n","\n","*   **Highlighting Potential Issues:** LangSmith's automatic evaluation metrics help identify potential problems or inconsistencies in your LLM's output. This allows you to prioritize manual review of specific examples rather than sifting through the entire dataset.\n","*   **Managing Complexity:**  LLM calls often involve complex combinations of user input, templates, and auxiliary functions. LangSmith helps you understand the exact input provided to the LLM, making it easier to pinpoint the source of any issues.\n","\n","**Improving Test Coverage:**\n","\n","*   **Testing Different Flows:** LangSmith supports testing both the overall flow of your application and individual LLM Chain or Chat Model components. This ensures comprehensive evaluation of your LLM's performance in various contexts.\n","*   **Scaling with Data Volume:** As your dataset grows, manual evaluation becomes impractical. LangSmith's tools enable you to efficiently manage and analyze large amounts of data, ensuring consistent and scalable testing. \n","\n"]}]},{"cell_type":"code","source":["demo_ephemeral_chat_history.add_ai_message(response[\"answer\"])\n","demo_ephemeral_chat_history.add_user_message(\"tell me more about that!\")\n","\n","response = conversational_retrieval_chain.invoke(\n","    {\"messages\": demo_ephemeral_chat_history.messages}\n",")\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wyPr_moRBNq","executionInfo":{"status":"ok","timestamp":1715157772903,"user_tz":-480,"elapsed":11990,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"5af98e06-d931-438a-9702-c6d4f7f6484f"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'messages': [HumanMessage(content='how can langsmith help with testing?'),\n","  AIMessage(content=\"## LangSmith's Role in Testing LLMs\\n\\nWhile LangSmith doesn't directly execute tests, it offers valuable tools to guide and enhance your LLM testing process, especially when dealing with large datasets.  Here's how:\\n\\n**Focusing Your Evaluation Efforts:**\\n\\n*   **Highlighting Potential Issues:** LangSmith's automatic evaluation metrics help identify potential problems or inconsistencies in your LLM's output. This allows you to prioritize manual review of specific examples rather than sifting through the entire dataset.\\n*   **Managing Complexity:**  LLM calls often involve complex combinations of user input, templates, and auxiliary functions. LangSmith helps you understand the exact input provided to the LLM, making it easier to pinpoint the source of any issues.\\n\\n**Improving Test Coverage:**\\n\\n*   **Testing Different Flows:** LangSmith supports testing both the overall flow of your application and individual LLM Chain or Chat Model components. This ensures comprehensive evaluation of your LLM's performance in various contexts.\\n*   **Scaling with Data Volume:** As your dataset grows, manual evaluation becomes impractical. LangSmith's tools enable you to efficiently manage and analyze large amounts of data, ensuring consistent and scalable testing. \\n\"),\n","  HumanMessage(content='tell me more about that!')],\n"," 'context': [Document(page_content='for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation‚ÄãInitially, we do most of our evaluation manually and ad hoc. We pass in different', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content=\"most of these evaluators aren't perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation‚ÄãAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n","  Document(page_content=\"associated with the dataset. From there, you can review them. We've made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we've added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we‚Äôre being honest,\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://web.archive.org/web/20231216051131/https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n"," 'answer': \"## Deep Dive into LangSmith's Testing Capabilities\\n\\n**Understanding the Context:**\\n\\n*   **Input Clarity:** LangSmith provides detailed logs of the exact input sent to your LLM, including user queries, tool outputs, and any intermediate steps. This transparency is crucial for understanding how different inputs influence the LLM's responses and identifying potential issues. \\n*   **Output Traceability:**  Similarly, LangSmith tracks the LLM's output at each stage, allowing you to analyze the reasoning process and pinpoint where errors or inconsistencies might occur. \\n\\n**Enhancing Evaluation:**\\n\\n*   **Automatic Metrics:** LangSmith offers a range of automatic evaluation metrics, such as BLEU score and ROUGE score, to assess the quality and coherence of LLM-generated text. These metrics provide a quantitative measure of performance, supplementing your qualitative analysis.\\n*   **Custom Metrics:** Beyond the built-in metrics, LangSmith allows you to define and track custom metrics tailored to your specific use case and evaluation criteria. This flexibility ensures that you're measuring what matters most for your application.\\n\\n**Streamlining the Process:**\\n\\n*   **Data Management:** LangSmith helps you organize and manage your testing data efficiently. You can easily filter and search through examples, making it convenient to focus on specific areas of interest or potential problems. \\n*   **Visualization:**  Visualizations like embedding plots help you understand the relationships between different inputs and outputs, providing insights into the LLM's behavior and potential biases.\\n\\n**Overall, LangSmith empowers you to:**\\n\\n*   **Identify and diagnose problems more effectively.**\\n*   **Improve the quality and reliability of your LLM applications.**\\n*   **Build trust and confidence in your LLM's capabilities.** \\n\"}"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["print(response['answer'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ge1RMVypRv19","executionInfo":{"status":"ok","timestamp":1715157775530,"user_tz":-480,"elapsed":308,"user":{"displayName":"Kearl Cordle","userId":"12869362405359631290"}},"outputId":"67953897-bada-46f0-c8c6-048170dbc17d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["## Deep Dive into LangSmith's Testing Capabilities\n","\n","**Understanding the Context:**\n","\n","*   **Input Clarity:** LangSmith provides detailed logs of the exact input sent to your LLM, including user queries, tool outputs, and any intermediate steps. This transparency is crucial for understanding how different inputs influence the LLM's responses and identifying potential issues. \n","*   **Output Traceability:**  Similarly, LangSmith tracks the LLM's output at each stage, allowing you to analyze the reasoning process and pinpoint where errors or inconsistencies might occur. \n","\n","**Enhancing Evaluation:**\n","\n","*   **Automatic Metrics:** LangSmith offers a range of automatic evaluation metrics, such as BLEU score and ROUGE score, to assess the quality and coherence of LLM-generated text. These metrics provide a quantitative measure of performance, supplementing your qualitative analysis.\n","*   **Custom Metrics:** Beyond the built-in metrics, LangSmith allows you to define and track custom metrics tailored to your specific use case and evaluation criteria. This flexibility ensures that you're measuring what matters most for your application.\n","\n","**Streamlining the Process:**\n","\n","*   **Data Management:** LangSmith helps you organize and manage your testing data efficiently. You can easily filter and search through examples, making it convenient to focus on specific areas of interest or potential problems. \n","*   **Visualization:**  Visualizations like embedding plots help you understand the relationships between different inputs and outputs, providing insights into the LLM's behavior and potential biases.\n","\n","**Overall, LangSmith empowers you to:**\n","\n","*   **Identify and diagnose problems more effectively.**\n","*   **Improve the quality and reliability of your LLM applications.**\n","*   **Build trust and confidence in your LLM's capabilities.** \n","\n"]}]},{"cell_type":"markdown","source":["您可以看到，用户的初始查询会直接传递给检索器，检索器会返回合适的文档。\n","\n","对后续问题的调用时会将用户的初始问题重新表述为与 LangSmith 测试更相关的问题，从而获得更高质量的文档。"],"metadata":{"id":"wcMLgAa9StUO"}}]}